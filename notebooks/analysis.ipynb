{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SR-Automation — Análise de Resultados\n",
    "\n",
    "Notebook para análise completa dos resultados do pipeline de revisão sistemática automatizada.\n",
    "Ref: Capítulo 4 da dissertação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Setup\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "with open(os.path.join(PROJECT_ROOT, \"config.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "OUTPUTS = os.path.join(PROJECT_ROOT, config[\"paths\"][\"outputs\"])\n",
    "DATA = os.path.join(PROJECT_ROOT, \"data\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    records = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                records.append(json.loads(line))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Outputs dir: {OUTPUTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Estatísticas do Corpus (§4.1)\n",
    "corpus_path = os.path.join(PROJECT_ROOT, config[\"paths\"][\"corpus\"])\n",
    "corpus = pd.read_csv(corpus_path)\n",
    "\n",
    "print(f\"Total de artigos: {len(corpus)}\")\n",
    "print(f\"\\nArtigos por fonte:\")\n",
    "print(corpus[\"source\"].value_counts())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Distribuição por ano\n",
    "corpus[\"year\"].value_counts().sort_index().plot(kind=\"bar\", ax=axes[0], color=\"steelblue\")\n",
    "axes[0].set_title(\"Distribuição por Ano\")\n",
    "axes[0].set_xlabel(\"Ano\")\n",
    "axes[0].set_ylabel(\"Artigos\")\n",
    "\n",
    "# Distribuição do tamanho dos abstracts\n",
    "corpus[\"abstract_len\"] = corpus[\"abstract\"].str.len()\n",
    "corpus[\"abstract_len\"].hist(bins=30, ax=axes[1], color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Tamanho dos Abstracts (caracteres)\")\n",
    "axes[1].set_xlabel(\"Caracteres\")\n",
    "axes[1].set_ylabel(\"Frequência\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUTS, \"figures\", \"corpus_stats.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Gold Standard (§4.1)\n",
    "gold_path = os.path.join(DATA, \"gold_standard.csv\")\n",
    "if os.path.exists(gold_path):\n",
    "    gold = pd.read_csv(gold_path)\n",
    "    print(f\"Gold standard: {len(gold)} artigos\")\n",
    "    \n",
    "    # Concordância inter-anotador\n",
    "    if \"reviewer_a\" in gold.columns and \"reviewer_b\" in gold.columns:\n",
    "        to_bin = lambda x: 1 if str(x).upper() in (\"YES\", \"INCLUDE\", \"1\") else 0\n",
    "        ra = gold[\"reviewer_a\"].apply(to_bin)\n",
    "        rb = gold[\"reviewer_b\"].apply(to_bin)\n",
    "        kappa = cohen_kappa_score(ra, rb)\n",
    "        print(f\"Cohen's Kappa inter-anotador: {kappa:.4f}\")\n",
    "        print(f\"  Alvo: ≥ {config['gold_standard']['min_kappa']}\")\n",
    "    \n",
    "    # Proporção\n",
    "    if \"consensus\" in gold.columns:\n",
    "        counts = gold[\"consensus\"].value_counts()\n",
    "        print(f\"\\nDistribuição do consenso:\")\n",
    "        print(counts)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        counts.plot(kind=\"pie\", autopct=\"%1.1f%%\", ax=ax, colors=[\"#2196F3\", \"#FF5722\"])\n",
    "        ax.set_title(\"Gold Standard: Inclusão vs Exclusão\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        plt.savefig(os.path.join(OUTPUTS, \"figures\", \"gold_standard_dist.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Gold standard não encontrado. Preencha data/gold_standard.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Resultados da Triagem (§4.2)\n",
    "triage_path = os.path.join(OUTPUTS, \"triage_results.jsonl\")\n",
    "triage = load_jsonl(triage_path)\n",
    "\n",
    "print(f\"Total triados: {len(triage)}\")\n",
    "print(f\"\\nDistribuição de decisões:\")\n",
    "print(triage[\"decision\"].value_counts())\n",
    "\n",
    "# Se métricas existem\n",
    "metrics_path = os.path.join(OUTPUTS, \"metrics.json\")\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(f\"\\n=== Métricas ===\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f} (alvo: ≥{config['evaluation']['recall_target']})\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"Workload Reduction: {metrics['workload_reduction_pct']:.1f}%\")\n",
    "    print(f\"Recall target met: {metrics['recall_target_met']}\")\n",
    "    \n",
    "    # Confusion matrix heatmap\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "    matrix = np.array([[cm[\"tp\"], cm[\"fn\"]], [cm[\"fp\"], cm[\"tn\"]]])\n",
    "    labels = np.array([[f'VP\\n{cm[\"tp\"]}', f'FN\\n{cm[\"fn\"]}'],\n",
    "                       [f'FP\\n{cm[\"fp\"]}', f'VN\\n{cm[\"tn\"]}']])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(matrix, annot=labels, fmt=\"\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Incluir\", \"Excluir\"],\n",
    "                yticklabels=[\"Incluir\", \"Excluir\"], ax=ax)\n",
    "    ax.set_xlabel(\"Predição do Sistema\")\n",
    "    ax.set_ylabel(\"Gold Standard\")\n",
    "    ax.set_title(\"Matriz de Confusão\")\n",
    "    plt.savefig(os.path.join(OUTPUTS, \"figures\", \"confusion_matrix_notebook.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Falsos negativos\n",
    "    fn_path = os.path.join(OUTPUTS, \"false_negatives_analysis.csv\")\n",
    "    if os.path.exists(fn_path):\n",
    "        fn = pd.read_csv(fn_path)\n",
    "        print(f\"\\nFalsos Negativos ({len(fn)}):\")\n",
    "        display(fn)\n",
    "else:\n",
    "    print(\"\\nMétricas não disponíveis. Execute: python main.py --step metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Cross-Validation (§4.5)\n",
    "cv_path = os.path.join(OUTPUTS, \"cross_validation.json\")\n",
    "if os.path.exists(cv_path):\n",
    "    with open(cv_path) as f:\n",
    "        cv = json.load(f)\n",
    "    \n",
    "    print(\"=== Cross-Validation ===\")\n",
    "    print(f\"Kappa R1-R2 (sinônimos): {cv['kappa_run1_run2']:.4f}\")\n",
    "    print(f\"Kappa R1-R3 (ordem inv.): {cv['kappa_run1_run3']:.4f}\")\n",
    "    print(f\"Kappa R2-R3: {cv['kappa_run2_run3']:.4f}\")\n",
    "    print(f\"Kappa médio: {cv['kappa_mean']:.4f} (esperado: ≥{cv['expected_kappa']})\")\n",
    "    print(f\"Concordância total: {cv['agreement_pct']:.1f}%\")\n",
    "    print(f\"Disagreements: {len(cv.get('disagreements', []))}\")\n",
    "    \n",
    "    # Tabela de kappas\n",
    "    kappa_data = {\n",
    "        \"Par\": [\"R1-R2\", \"R1-R3\", \"R2-R3\", \"Média\"],\n",
    "        \"Kappa\": [cv[\"kappa_run1_run2\"], cv[\"kappa_run1_run3\"],\n",
    "                  cv[\"kappa_run2_run3\"], cv[\"kappa_mean\"]],\n",
    "    }\n",
    "    display(pd.DataFrame(kappa_data))\n",
    "    \n",
    "    if cv.get(\"disagreements\"):\n",
    "        print(f\"\\nDisagreements:\")\n",
    "        display(pd.DataFrame(cv[\"disagreements\"]))\n",
    "else:\n",
    "    print(\"Cross-validation não disponível. Execute: python main.py --step crossval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Extração e Sumarização (§4.3)\n",
    "ext_path = os.path.join(OUTPUTS, \"extraction_results.jsonl\")\n",
    "sum_path = os.path.join(OUTPUTS, \"summaries.jsonl\")\n",
    "\n",
    "if os.path.exists(ext_path):\n",
    "    extractions = load_jsonl(ext_path)\n",
    "    print(f\"Extrações: {len(extractions)} artigos\")\n",
    "    \n",
    "    # Taxa de parse_error\n",
    "    parse_errors = extractions.get(\"parse_error\", pd.Series([False]*len(extractions))).sum()\n",
    "    print(f\"Parse errors: {parse_errors} ({parse_errors/len(extractions)*100:.1f}%)\")\n",
    "    \n",
    "    # Campos NOT MENTIONED\n",
    "    fields = [\"study_objective\", \"methodology\", \"main_results\",\n",
    "              \"conclusions_limitations\", \"sample_data\"]\n",
    "    for field in fields:\n",
    "        if field in extractions.columns:\n",
    "            nm = (extractions[field] == \"NOT MENTIONED\").sum()\n",
    "            print(f\"  {field}: {nm} NOT MENTIONED ({nm/len(extractions)*100:.1f}%)\")\n",
    "    \n",
    "    # Exemplo\n",
    "    print(\"\\n--- Exemplo de extração ---\")\n",
    "    sample = extractions.iloc[0]\n",
    "    for field in fields:\n",
    "        if field in sample:\n",
    "            print(f\"  {field}: {sample[field][:100]}...\" if len(str(sample.get(field, ''))) > 100 else f\"  {field}: {sample.get(field, '')}\")\n",
    "\n",
    "if os.path.exists(sum_path):\n",
    "    summaries = load_jsonl(sum_path)\n",
    "    print(f\"\\nSumarizações: {len(summaries)} artigos\")\n",
    "    \n",
    "    # Exemplo\n",
    "    print(\"\\n--- Exemplo de sumarização ---\")\n",
    "    sample = summaries.iloc[0]\n",
    "    print(f\"  Problem: {sample.get('problem', 'N/A')}\")\n",
    "    print(f\"  Solution: {sample.get('solution', 'N/A')}\")\n",
    "    print(f\"  Findings: {sample.get('findings', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7: Eficiência Operacional (§4.4)\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    total_time = metrics[\"total_time_seconds\"]\n",
    "    manual_time = metrics[\"manual_baseline_seconds\"]\n",
    "    reduction = metrics[\"time_reduction_pct\"]\n",
    "    tokens = metrics[\"total_tokens\"]\n",
    "    \n",
    "    print(\"=== Eficiência Operacional ===\")\n",
    "    print(f\"Tempo pipeline: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    print(f\"Baseline manual: {manual_time:.1f}s ({manual_time/60:.1f} min)\")\n",
    "    print(f\"Redução de tempo: {reduction:.1f}%\")\n",
    "    print(f\"Tokens consumidos: {tokens:,}\")\n",
    "    \n",
    "    # Gráfico comparativo\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    methods = [\"Pipeline\\nAutomatizado\", \"Processo\\nManual\"]\n",
    "    times_min = [total_time / 60, manual_time / 60]\n",
    "    colors = [\"#2196F3\", \"#FF5722\"]\n",
    "    ax.bar(methods, times_min, color=colors)\n",
    "    ax.set_ylabel(\"Tempo (minutos)\")\n",
    "    ax.set_title(\"Comparação de Tempo: Automatizado vs Manual\")\n",
    "    for i, v in enumerate(times_min):\n",
    "        ax.text(i, v + 0.5, f\"{v:.1f} min\", ha=\"center\", fontweight=\"bold\")\n",
    "    plt.savefig(os.path.join(OUTPUTS, \"figures\", \"time_comparison.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Métricas não disponíveis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8: Alucinação (§4.5)\n",
    "hall_path = os.path.join(OUTPUTS, \"hallucination_sample.csv\")\n",
    "if os.path.exists(hall_path):\n",
    "    hall = pd.read_csv(hall_path)\n",
    "    classified = hall[hall[\"classification\"].notna() & (hall[\"classification\"] != \"\")]\n",
    "    \n",
    "    if not classified.empty:\n",
    "        print(\"=== Taxa de Alucinação ===\")\n",
    "        counts = classified[\"classification\"].str.upper().value_counts()\n",
    "        print(counts)\n",
    "        \n",
    "        total = len(classified)\n",
    "        hall_count = counts.get(\"HALLUCINATED\", 0)\n",
    "        print(f\"\\nHallucination rate: {hall_count/total*100:.2f}%\")\n",
    "        \n",
    "        # Por módulo\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        for module in [\"triage\", \"extraction\", \"summarization\"]:\n",
    "            mod_df = classified[classified[\"module\"] == module]\n",
    "            if not mod_df.empty:\n",
    "                mod_counts = mod_df[\"classification\"].str.upper().value_counts()\n",
    "                print(f\"\\n{module}:\")\n",
    "                print(mod_counts)\n",
    "        \n",
    "        # Gráfico\n",
    "        by_module = classified.groupby([\"module\", \"classification\"]).size().unstack(fill_value=0)\n",
    "        by_module.plot(kind=\"bar\", ax=ax, colormap=\"Set2\")\n",
    "        ax.set_title(\"Classificação de Claims por Módulo\")\n",
    "        ax.set_xlabel(\"Módulo\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(title=\"Classificação\")\n",
    "        plt.savefig(os.path.join(OUTPUTS, \"figures\", \"hallucination_by_module.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Amostra gerada ({len(hall)} claims) mas sem classificação.\")\n",
    "        print(\"Preencha a coluna 'classification' em hallucination_sample.csv\")\n",
    "        print(\"Valores: GROUNDED | INFERRED | HALLUCINATED\")\n",
    "else:\n",
    "    print(\"Amostra de alucinação não encontrada. Execute: python main.py --step hallcheck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 9: Avaliação de Resumos (§4.3)\n",
    "eval_path = os.path.join(OUTPUTS, \"summary_evaluation_template.csv\")\n",
    "if os.path.exists(eval_path):\n",
    "    eval_df = pd.read_csv(eval_path)\n",
    "    # Filtrar apenas linhas preenchidas\n",
    "    filled = eval_df[eval_df[\"clarity\"].notna()]\n",
    "    \n",
    "    if not filled.empty:\n",
    "        dims = [\"clarity\", \"completeness\", \"accuracy\", \"utility\"]\n",
    "        \n",
    "        print(\"=== Avaliação de Resumos (Likert 1-5) ===\")\n",
    "        for dim in dims:\n",
    "            if dim in filled.columns:\n",
    "                print(f\"  {dim}: média={filled[dim].mean():.2f}, std={filled[dim].std():.2f}\")\n",
    "        \n",
    "        # Kappa inter-avaliadores (se 2 avaliadores)\n",
    "        evaluators = filled[\"evaluator\"].unique()\n",
    "        if len(evaluators) >= 2:\n",
    "            e1 = filled[filled[\"evaluator\"] == evaluators[0]]\n",
    "            e2 = filled[filled[\"evaluator\"] == evaluators[1]]\n",
    "            merged = e1.merge(e2, on=\"article_id\", suffixes=(\"_1\", \"_2\"))\n",
    "            for dim in dims:\n",
    "                if f\"{dim}_1\" in merged.columns:\n",
    "                    kappa = cohen_kappa_score(\n",
    "                        merged[f\"{dim}_1\"].astype(int),\n",
    "                        merged[f\"{dim}_2\"].astype(int),\n",
    "                        weights=\"linear\"\n",
    "                    )\n",
    "                    print(f\"  Kappa {dim}: {kappa:.4f}\")\n",
    "        \n",
    "        # Box plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        filled[dims].boxplot(ax=ax)\n",
    "        ax.set_title(\"Avaliação de Resumos por Dimensão (Likert 1-5)\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_ylim(0.5, 5.5)\n",
    "        plt.savefig(os.path.join(OUTPUTS, \"figures\", \"summary_evaluation.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Template de avaliação encontrado mas sem dados.\")\n",
    "        print(\"Preencha outputs/summary_evaluation_template.csv com scores Likert 1-5.\")\n",
    "else:\n",
    "    print(\"Template de avaliação não encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 10: Exportação\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "from src.report_generator import (\n",
    "    generate_confusion_matrix_plot,\n",
    "    generate_latex_tables,\n",
    "    generate_metrics_bar_chart,\n",
    ")\n",
    "\n",
    "metrics_path = os.path.join(OUTPUTS, \"metrics.json\")\n",
    "crossval_path = os.path.join(OUTPUTS, \"cross_validation.json\")\n",
    "\n",
    "if os.path.exists(metrics_path):\n",
    "    generate_latex_tables(metrics_path, crossval_path, config)\n",
    "    generate_confusion_matrix_plot(metrics_path, config)\n",
    "    generate_metrics_bar_chart(metrics_path, config)\n",
    "    print(\"Exportação concluída!\")\n",
    "    print(f\"  Tabelas LaTeX: {os.path.join(OUTPUTS, 'latex_tables.tex')}\")\n",
    "    print(f\"  Figuras: {os.path.join(OUTPUTS, 'figures/')}\")\n",
    "else:\n",
    "    print(\"Métricas não disponíveis para exportação.\")\n",
    "    print(\"Execute o pipeline completo primeiro.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
