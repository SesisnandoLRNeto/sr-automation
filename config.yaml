model:
  provider: "groq"
  model_name: "llama-3.3-70b-versatile"
  fallback_provider: "together"
  fallback_model: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"

inference:
  triage:
    temperature: 0.1
    top_p: 0.9
    max_tokens: 100
  extraction:
    temperature: 0.0
    top_p: 1.0
    max_tokens: 1024
  summarization:
    temperature: 0.3
    top_p: 0.95
    max_tokens: 512
    repetition_penalty: 1.1

embeddings:
  model: "all-MiniLM-L6-v2"
  device: "cpu"

vector_store:
  backend: "chromadb"
  collection_name: "sr_corpus"
  persist_directory: "data/processed/embeddings"

reranking:
  model: "cross-encoder/ms-marco-MiniLM-L-12-v2"
  top_k: 20

corpus:
  query: '"intelligent tutoring system" AND ("large language model" OR "LLM" OR "GPT" OR "transformer")'
  year_range: [2022, 2024]
  language: "en"
  target_size: [80, 120]
  dedup_threshold: 0.9
  sources: ["semantic_scholar", "openalex", "arxiv", "eric"]

gold_standard:
  min_kappa: 0.7
  criteria:
    - "Presents an intelligent tutoring system or educational tool"
    - "Uses an LLM as a central component"
    - "Reports empirical evaluation or proof of concept"

cross_validation:
  num_runs: 3
  expected_kappa: 0.8

hallucination_check:
  sample_size_per_module: 30

evaluation:
  recall_target: 0.85
  recall_aspiration: 0.90
  manual_baseline_minutes_per_article: 5

paths:
  corpus: "data/raw/corpus.csv"
  gold_standard: "data/gold_standard.csv"
  outputs: "outputs/"
  prompts: "prompts/"
